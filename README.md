# Operationalizing AWS ML Project
As part of requirements for the AWS MLEND

## Udacity Project 4: AWS Machine Learning Engineer Nanodegree Program (2021-2022)

In this proyect we are requested to accomplish five steps towards applying accuired skills
about most importan services that AWS offer to the operative part of training and deploying 
Machine Learning models.

The project consists in 5 Steps
Step 1: Train and deploy a model on a Sagemaker notebook (one of the modalities that Sagemaker offer amongst several more).
Step 2: Perform a similar task on an EC2 instance.
Step 3: Create a Lambda function that will consume your model inference capabilites via endpoints. 
Step 4: Set up auto-scaling for your deployed
endpoint as well as concurrency for your Lambda function.
â€¢ Ensure that the security on your ML pipeline is set up properly.


## Step 0: Project Set Up and Installation
You must be logged to a AWS account and search for SageMaker to get into the SageMaker console. 
Launch Sagemaker Studio.
It could take a few minutes to start. 
Once open you will be able to upload all the files of this project and run the Jupyter Notebook that comprises the whole process described above.


The files to upload are: 
`train_adn_deploy.ipynb`, running each cell will execute the whole process and three auxiliary python scripts that are called by the notebook:
`hpo.py`, contains the prediction model, the training loop as well as the validation and testing tasks in step 2 for hyperparameter optimization.
`train_model.py`, esentially identical to hpo.py, but with the hooks of SageMaker module that performs debugging of the model in step 3.
endpoint_inference.py responsible for invoking the endpoint created by the notebook and return the prediction.

Open the Jupyter Notebook and select the kernel as follows:
<br/>
<img src="images/kernel.png" width="50%">
<br/>
<br/>
and the instance:<br/>
<img src="images/instance.png" width="50%">
<br/>




## Step 1: Data used
The first step in the notebook is to get data from the web, unzip it, and make it available into s3.
You must generate an s3 bucket before and provide the bucket name to the corresponding cell on the notebook.
Also you must use the region global for the s3 bucket, but the region US-east-1 (N.Virginia) for the SageMaker studio.
The dataset is available at https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip 
and the notebook gets it into the SageMaker filesystem by executing the linux command wget from the cell. 


## Step 2: Hyperparameter Optimization
The next part of the notebook deals with hyperparameter optimization. For that task we provide a range of the hyperparameters to investigate and the cells 
run the script `hpo.py`.
This script receives the arguments from the cell and performs the train, validation and testing processes to obtain the best combination of hyperparameters.
Then those hyperparameters are passed to the third step.

## Step 3: Debugging and Profiling
<br/>*Debugging*<br/>
Now we will use hooks from the SageMaker module that *automagically* corrects weights to avoid undesirable effects as 
`vanishing gradients, overfitting, overtraining or poor weight initialization`
Finally the plot of the Cross Entropy Loss is shown in a cell:
<br/>
<img src="images/crossentropy.png" width="80%">
<br/><br/>

This graph serve to see if a bad behavior like the blue line with erratic oscillations or not decreasing with batches. 
In that case batches could be shuffled to try to minimize this effect.

<br/>*Profiling*<br/>
The profiler report is generated by the SageMaker debugger and can be downloaded from [here](profiler/profiler-report.html) to be seen in your browser.
<br/>


## Step 4: Model Deployment
Model was deployed to a "ml.m5.large" instance because of low costs for having this endpoint running. 
Several dog's images were sent to the endpoint to perform a prediction and see if the model is able to give back the breed's dog.
See last part of the notebook to check results.

## Final words:
AWS was able to let us quickly create this model and deploy it in a way that is easily scalable. This is one of the strongest features of the AWS Sagemaker
for quick and professional Machine Learning solutions.
